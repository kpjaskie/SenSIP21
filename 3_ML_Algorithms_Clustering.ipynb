{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Algorithms - K-means",
      "provenance": [],
      "authorship_tag": "ABX9TyOL0GbaJK696vX6Cdxt0+L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpjaskie/SenSIP21/blob/main/3_ML_Algorithms_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reKXmsGDRwP_"
      },
      "source": [
        "#K-Means CLustering\n",
        "The K-Means clustering algorithm is an **unsupervised** learning algorithm that finds underlying structure in unlabeled data.  In this workbook, we will explore the K-means algorithm and use it to cluster data in provided datasets.\n",
        "\n",
        "While the implementation of a basic K-Means algorithm is not difficult (and we will explore it as well in this notebook), we can find built-in algorithms using the Scikit-Learn library.  This is what we will start with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYwhnSIeYEXX"
      },
      "source": [
        "# First, we need to import the relavent libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdcWwEhZNnN"
      },
      "source": [
        "Before we can apply K-means, we need a dataset to apply it to.  Later we will use some real-world datasets, but for now we can generate a random dataset composed of random \"blobs\" or clusters of data.  \n",
        "\n",
        "Notice that while the vertical axis is often labeled *y* in mathematics, in Machine Learning *y* typically represents the data label.  The data is typically stored entirely in *X*, which is commonly a matrix with multiple columns.  In this case, we are creating two-dimensional data, so *X* has two columns.  Here, *y* is a single vector containing 0's, 1's, and 2's representing which blob each datapoint belongs to.  We will in fact be estimating this *y* during this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7AynWzTZeYk"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0) #If you remove or change the random_state=0, it will create random clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRL27YY4e8P5"
      },
      "source": [
        "Once we have our data (stored in variable X), we can apply the built-in Scikit-Learn K-means algorithm.  Since we can see that there are three clusters in the data, we will tell the K-means algorithm to find 3 clusters.  The number of clusters the K-means algorithm looks for is a **hyperparameter**.  Here, I'm telling the algorithm to choose three random points for my initial cluster centroids.\n",
        "\n",
        "\n",
        "\n",
        "*   The `centroids` variable returns the mean value for each cluster. In this case, there will be three centroids.\n",
        "*   Notice here that I've stored the calculated cluster labels in a variable called `y_hat`.  We typically label variable estimates with a hat:\n",
        "\n",
        "$$Estimate\\space of\\space y = \\hat{y}$$\n",
        "\n",
        "*   The returned `cost` is also sometimes called the inertia.  This is the sum of squares of the distance of each datapoint to its cluster centroid.  \n",
        "\n",
        "$$cost=arg\\space \\min_{C} \\sum_{c=1}^{k} \\sum_{i\\in c} ||x_i-\\mu_c||^2 $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTbIYGN-b-Vg"
      },
      "source": [
        "from sklearn.cluster import k_means\n",
        "\n",
        "# Perform k-means clustering\n",
        "centroids, y_hat, cost = k_means(X, n_clusters=3)\n",
        "\n",
        "# Print output\n",
        "print('\\nCentroids = \\n', centroids)\n",
        "print('\\nFirst couple y_hat = ', y_hat[:10])\n",
        "print('\\nCost = ', cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7hpkRgVjk6G"
      },
      "source": [
        "Once we've applied the algorithm, we can plot the resulting clusters and their centroids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6Hly2zfG4I"
      },
      "source": [
        "# Plot the now labeled clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_hat, s=50) \n",
        "\n",
        "# Plot the centroids\n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1diRxIxqG2a"
      },
      "source": [
        "Congratulations!  You've performed your first K-Means clustering algorithm.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsUG8uk9sBN7"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "Notice that we told the k-means algorithm to use 3 clusters, because that is what we ourselves can see.  In a more complex problem, this will not be possible.  We would like to *tune* this **hyperparameter** to identify the best possible value even when we are unable to visualize the clusters ourselves.\n",
        "\n",
        "To do this, we want to try several possible numbers of clusters and plot the cost per clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DxWvUVFsAxp"
      },
      "source": [
        "cluster_nums = np.arange(start=2, stop=10)\n",
        "costs = np.empty_like(cluster_nums)\n",
        "\n",
        "for c in cluster_nums:\n",
        "    centroids, y_hat, cost = k_means(X, n_clusters=c)\n",
        "    costs[c-2] = cost\n",
        "\n",
        "plt.plot(cluster_nums, costs)\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Cost')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwSqkzhrxvY5"
      },
      "source": [
        "As we can see from this plot, there is a distinct *elbow* at 3 clusters.  This elbow point indicates the \"inherant\" number of clusters in the data.  \n",
        "\n",
        "Sometimes there won't be an elbow.  This can happen if the data is continuous and not inherantly broken into clusters.  Clustering in this situation can still be useful, though the user must choose the number of clusters based on their interests. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1TZ_y4cyo3j"
      },
      "source": [
        "# K-Means Algorithm\n",
        "Since the K-means algorithm is so simple, we will also provide it here.  It will allow us to see the steps of the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRBtKxh_qE71"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin\n",
        "\n",
        "def find_clusters(X, n_clusters, rseed=2, disp_steps=True):\n",
        "    # Randomly choose initialization clusters\n",
        "    rng = np.random.RandomState(rseed)\n",
        "    init_centroids = rng.permutation(X.shape[0])[:n_clusters]\n",
        "    centroids = X[init_centroids]\n",
        "    \n",
        "    still_running = True\n",
        "    itteration = 1\n",
        "    while still_running == True:\n",
        "        # Assign labels based on closest centroid\n",
        "        labels = pairwise_distances_argmin(X, centroids)\n",
        "        \n",
        "        # Find new centroids from means of points\n",
        "        new_centroids = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
        "        \n",
        "        # Check for convergence\n",
        "        if np.all(centroids == new_centroids):\n",
        "            still_running = False\n",
        "        centroids = new_centroids\n",
        "\n",
        "        if disp_steps == True:\n",
        "            plt.figure(itteration-1)\n",
        "            plt.scatter(X[:, 0], X[:, 1], c=labels, s=50)\n",
        "            plt.title('Itteration ' + str(itteration))\n",
        "            plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, alpha=0.5)\n",
        "\n",
        "        itteration = itteration + 1\n",
        "\n",
        "    return centroids, labels\n",
        "\n",
        "centroids, labels = find_clusters(X, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPxlnKg3BK1Q"
      },
      "source": [
        "#Optimality\n",
        "It is important to know that the K-means algorithm is not guarenteed to produce the *optimal* clustering (the clustering with the lowest cost).  Each step will always produce a *better* clustering, but it is possible for the algorithm to end at what is called a *local optimum* rather than a *global optimum*.\n",
        "\n",
        "To handle this situation, it is common to run the K-means algorithm from several different initial starting locations.  The Scikit-Learn k_means function defaults to choosing 10 different random initial starting locations and choosing the solution that has the lowest cost.\n",
        "\n",
        "Below, we show an example where our k-means implementation gets stuck in a local optimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB_zjZvGl2Qv"
      },
      "source": [
        "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "centroids, labels = find_clusters(X, 4, rseed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjivk8PKKYKe"
      },
      "source": [
        "#Linearity\n",
        "\n",
        "You may have noticed that K-means divides the plane up linearly - the boundaries between clusters being straight lines.  This means that more complex, non-linear clusters are not clustered properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p2K2Y9rFbay"
      },
      "source": [
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcklhevWLKdf"
      },
      "source": [
        "centroids, y_hat, cost = k_means(X, n_clusters=2)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_hat, s=50) \n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrosUC2hLXEd"
      },
      "source": [
        "To deal with this situation, a more complex algorithm that uses *kernels* is needed.  One such algorithm is called **Spectral Clustering**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUxVP40dLt5D"
      },
      "source": [
        "from sklearn.cluster import SpectralClustering\n",
        "model = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n",
        "                           assign_labels='kmeans')\n",
        "labels = model.fit_predict(X)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69KMiWc_PAfg"
      },
      "source": [
        "# Real World Clustering Application\n",
        "\n",
        "In this example, we're going to cluster handwritten digits.  We are going to ignore the known labels and attempt to just cluster the images using a K-means algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx1vhR5xUYhR"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()\n",
        "\n",
        "X = digits.data\n",
        "\n",
        "print(\"We have \" + str(X.shape[0]) + \" handwritten digits and each digit has \" \n",
        "      + str(X.shape[1]) + \" pixels/features.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jxq__fxaVMYe"
      },
      "source": [
        "We know there are 10 digits in the dataset, so we can perform a K-means clustering as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5bcko6PVDds"
      },
      "source": [
        "centroids, y_hat, cost = k_means(X, n_clusters=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUHgz_YMXHJV"
      },
      "source": [
        "Now lets plot the cluster centroids.  Remember, the centroids are located at the mean of each cluster.  In this case, the data has 64 dimensions - one for each pixel.  Each handwritten digit can be thought of as a single point in 64 dimensional space.  The centroid for each cluster will also be a point in 64 dimensional space.  We're going to reshape it into an image and show the images those centroids represent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9UwvVgBWktA"
      },
      "source": [
        "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
        "centroids_img = centroids.reshape(10, 8, 8)\n",
        "for axi, centroids_img in zip(ax.flat, centroids_img):\n",
        "    axi.set(xticks=[], yticks=[])\n",
        "    axi.imshow(centroids_img, interpolation='nearest', cmap=plt.cm.binary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvssVbL_Yfv2"
      },
      "source": [
        "Notice that we performed **unsupervised clustering** meaning we ignored the labels that came with the data.  Regardless, the digits are all recognizable except for the 8.  We can give each digit in each cluster the estimated label based on the cluster image, and see how accurate this K-means clustering was at clustering the digits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFtKG4k_YguQ"
      },
      "source": [
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "labels = np.zeros_like(y_hat)\n",
        "for i in range(10):\n",
        "    mask = (y_hat == i)\n",
        "    labels[mask] = mode(digits.target[mask])[0]\n",
        "\n",
        "accuracy = accuracy_score(digits.target, labels)\n",
        "print(\"We were able to cluster digits with \" + str(round(accuracy * 100, 2)) + \"% accuracy.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNRJx3zAZ2fN"
      },
      "source": [
        "We got a pretty high accuracy from just a simple K-means algorithm!  Let's take a look at the Confusion Matrix to help see what's going on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1sL_DlgaAT1"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns; sns.set()  # for plot styling\n",
        "\n",
        "CM = confusion_matrix(digits.target, labels)\n",
        "sns.heatmap(CM.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=digits.target_names,\n",
        "            yticklabels=digits.target_names)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfIL0IDic1RZ"
      },
      "source": [
        "#K-Means for Image Compression\n",
        "\n",
        "An interesting use of the K-means algorithm is compression.  We can simplify something like an image that has millions of colors into one with many fewer colors with only a mild reduction in quality.  This is sometimes kown as quantization and is used in other areas such as speech compression as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBIFDi-0dR17"
      },
      "source": [
        "from sklearn.datasets import load_sample_image\n",
        "china = load_sample_image(\"china.jpg\")\n",
        "ax = plt.axes(xticks=[], yticks=[])\n",
        "ax.imshow(china);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scU8BAs5eEiI"
      },
      "source": [
        "First, we need to reshape the data to be in the right format.  We're going to flatten it.  The data as given is in three dimensions - the x and y coordinates plus a third dimension for the color (containing the Red, Green, and Blue values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8vqfhoOeAlg"
      },
      "source": [
        "china.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cD6V5R5ebTM"
      },
      "source": [
        "# Here we're going to normalize the data\n",
        "data = china / 255.0 # use 0...1 scale\n",
        "\n",
        "# Then reshape it\n",
        "data = data.reshape(427 * 640, 3)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM_9wNgahhTB"
      },
      "source": [
        "Here we're going to use a variant of the K-means function we used above.  This MiniBatchKMeans is useful when the dataset is extremely large.  Optimization methods are used to speed up the run-time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGACOoR_e6MQ"
      },
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "num_colors = 16\n",
        "\n",
        "kmeans = MiniBatchKMeans(num_colors)\n",
        "kmeans.fit(data)\n",
        "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDy26JH7hx3Q"
      },
      "source": [
        "Once we have performed the K-means to segment the original 16 million colorspace into a smaller set (specified by num_colors above), we can now reconstruct our image using the smaller colorspace that we created using K-means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FDoUArifJIU"
      },
      "source": [
        "china_recolored = new_colors.reshape(china.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
        "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].imshow(china)\n",
        "ax[0].set_title('16 million-color Original Image', size=16)\n",
        "ax[1].imshow(china_recolored)\n",
        "ax[1].set_title(str(num_colors) + '-color Image', size=16);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}