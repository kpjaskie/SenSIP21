{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Algorithms - K-means",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPLCNlDwlgG4ulecA1jyjpG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpjaskie/SenSIP21/blob/main/3_ML_Algorithms_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reKXmsGDRwP_"
      },
      "source": [
        "# K-Means CLustering\n",
        "The K-Means clustering algorithm is an **unsupervised** learning algorithm that finds underlying structure in unlabeled data.  In this workbook, we will explore the K-means algorithm and use it to cluster data in provided datasets.\n",
        "\n",
        "While the implementation of a basic K-Means algorithm is not difficult (and we will explore it as well in this notebook), we can find built-in algorithms using the Scikit-Learn library.  This is what we will start with.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYwhnSIeYEXX"
      },
      "source": [
        "# First, we need to import the relavent libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEdcWwEhZNnN"
      },
      "source": [
        "Before we can apply K-means, we need a dataset to apply it to.  Later we will use some real-world datasets, but for now we can generate a random dataset composed of random \"blobs\" or clusters of data.  \n",
        "\n",
        "Notice that while the vertical axis is often labeled *y* in mathematics, in Machine Learning *y* typically represents the data label.  The data is typically stored entirely in *X*, which is commonly a matrix with multiple columns.  In this case, we are creating two-dimensional data, so *X* has two columns.  Here, *y* is a single vector containing 0's, 1's, and 2's representing which blob each datapoint belongs to.  We will in fact be estimating this *y* during this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7AynWzTZeYk"
      },
      "source": [
        "#@title Generate random data\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "Dataset_Size = 300 #@param {type:\"slider\", min:50, max:1000, step:10}\n",
        "Number_of_Clusters = 3 #@param {type:\"slider\", min:0, max: 5, step:1}\n",
        "Standard_Deviation = 0.6 #@param {type:\"slider\", min:0, max:2, step:0.1}\n",
        "Random_Seed = 0 #@param {type:\"integer\"}\n",
        "#@markdown Different clusters can be obtained using different random seeds.\n",
        "\n",
        "#If you remove the random_state=0, it will create random clusters\n",
        "X, y = make_blobs(n_samples=Dataset_Size, \n",
        "                  centers=Number_of_Clusters, \n",
        "                  cluster_std=Standard_Deviation, \n",
        "                  random_state=Random_Seed) \n",
        "\n",
        "#Display the blobs\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRL27YY4e8P5"
      },
      "source": [
        "Once we have our data (stored in variable X), we can apply the built-in Scikit-Learn K-means algorithm.  Since we can see that there are three clusters in the data, we will tell the K-means algorithm to find 3 clusters.  The number of clusters the K-means algorithm looks for is a **hyperparameter**.  Here, I'm telling the algorithm to choose three random points for my initial cluster centroids.\n",
        "\n",
        "\n",
        "\n",
        "*   The `centroids` variable returns the mean value for each cluster. In this case, there will be three centroids.\n",
        "*   Notice here that I've stored the calculated cluster labels in a variable called `y_hat`.  We typically label variable estimates with a hat:\n",
        "\n",
        "$$Estimate\\space of\\space y = \\hat{y}$$\n",
        "\n",
        "*   The returned `cost` is also sometimes called the inertia.  This is the sum of squares of the distance of each datapoint to its cluster centroid.  \n",
        "\n",
        "$$cost=arg\\space \\min_{C} \\sum_{c=1}^{k} \\sum_{i\\in c} ||x_i-\\mu_c||^2 $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTbIYGN-b-Vg"
      },
      "source": [
        "#@title K-Means Clustering using sklearn\n",
        "from sklearn.cluster import k_means\n",
        "\n",
        "# Perform k-means clustering\n",
        "centroids, y_hat, cost = k_means(X, n_clusters=3)\n",
        "\n",
        "# Print output\n",
        "print('\\nCentroids = \\n', centroids)\n",
        "print('\\nFirst couple y_hat = ', y_hat[:10])\n",
        "print('\\nCost = ', cost)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7hpkRgVjk6G"
      },
      "source": [
        "Once we've applied the algorithm, we can plot the resulting clusters and their centroids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh6Hly2zfG4I"
      },
      "source": [
        "#@title Plot the labeled clusters\n",
        "plt.scatter(X[:, 0], X[:, 1], \n",
        "            c=y_hat, s=50) #here the color is based on y_hat\n",
        "\n",
        "# Plot the centroids\n",
        "plt.scatter(centroids[:, 0], \n",
        "            centroids[:, 1], \n",
        "            c='black', \n",
        "            s=100, \n",
        "            alpha=0.5)  #This is the level of transparency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1diRxIxqG2a"
      },
      "source": [
        "Congratulations!  You've performed your first K-Means clustering algorithm.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsUG8uk9sBN7"
      },
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "Notice that we told the k-means algorithm to use 3 clusters, because that is what we ourselves can see.  In a more complex problem, this will not be possible.  We would like to *tune* this **hyperparameter** to identify the best possible value even when we are unable to visualize the clusters ourselves.\n",
        "\n",
        "To do this, we want to try several possible numbers of clusters and plot the cost per clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g9PqVUby9lm"
      },
      "source": [
        "cluster_nums = np.arange(start=2, stop=10)\n",
        "cluster_nums"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DxWvUVFsAxp"
      },
      "source": [
        "#@title Create a Cost Plot\n",
        "costs = np.empty_like(cluster_nums) #starts at 0\n",
        "\n",
        "for c in cluster_nums:\n",
        "    centroids, y_hat, cost = k_means(X, n_clusters=c)\n",
        "    costs[c-2] = cost #store value into appropriate index\n",
        "\n",
        "plt.plot(cluster_nums, costs)\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Cost')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwSqkzhrxvY5"
      },
      "source": [
        "As we can see from this plot, there is a distinct *elbow* at 3 clusters.  This elbow point indicates the \"inherant\" number of clusters in the data.  \n",
        "\n",
        "Sometimes there won't be an elbow.  This can happen if the data is continuous and not inherantly broken into clusters.  Clustering in this situation can still be useful, though the user must choose the number of clusters based on their interests. \n",
        "\n",
        "## Give it a Try:\n",
        "Try changing the number of clusters and the standard deviation of each cluster in the generated data to see how this changes. You'll notice that if your data has a high enough standard deviation to start overlapping, this plot will start to have a less obvious elbow in it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1TZ_y4cyo3j"
      },
      "source": [
        "# How the K-Means Algorithm Works\n",
        "Since the K-means algorithm is so simple, we will also provide it here.  It will allow us to see the steps of the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRBtKxh_qE71"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances_argmin\n",
        "\n",
        "def find_clusters(X, n_clusters, rseed=2, disp_steps=True):\n",
        "    # Randomly choose initialization clusters\n",
        "    rng = np.random.RandomState(rseed)\n",
        "    init_centroids = rng.permutation(X.shape[0])[:n_clusters]\n",
        "    centroids = X[init_centroids]\n",
        "\n",
        "    # Illustrate the original cluster centroids\n",
        "    plt.figure(0)\n",
        "    plt.scatter(X[:, 0], X[:, 1], s=50)\n",
        "    plt.title('Itteration 0')\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, alpha=0.5)\n",
        "    \n",
        "    still_running = True\n",
        "    itteration = 2\n",
        "    while still_running == True:\n",
        "        # Assign labels based on closest centroid\n",
        "        labels = pairwise_distances_argmin(X, centroids)\n",
        "        \n",
        "        # Find new centroids from means of points\n",
        "        new_centroids = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
        "        \n",
        "        # Check for convergence\n",
        "        if np.all(centroids == new_centroids):\n",
        "            still_running = False\n",
        "        centroids = new_centroids\n",
        "\n",
        "        if disp_steps == True:\n",
        "            plt.figure(itteration-1)\n",
        "            plt.scatter(X[:, 0], X[:, 1], c=labels, s=50)\n",
        "            plt.title('Itteration ' + str(itteration))\n",
        "            plt.scatter(centroids[:, 0], centroids[:, 1], c='black', s=100, alpha=0.5)\n",
        "\n",
        "        itteration = itteration + 1\n",
        "\n",
        "    return centroids, labels\n",
        "\n",
        "#Call the find_clusters algorithm to see what's happening \n",
        "centroids, labels = find_clusters(X, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPxlnKg3BK1Q"
      },
      "source": [
        "#Optimality\n",
        "It is important to know that the K-means algorithm is not guarenteed to produce the *optimal* clustering (the clustering with the lowest cost).  Each step will always produce a *better* clustering, but it is possible for the algorithm to end at what is called a *local optimum* rather than a *global optimum*.\n",
        "\n",
        "To handle this situation, it is common to run the K-means algorithm from several different initial starting locations.  The Scikit-Learn k_means function defaults to choosing 10 different random initial starting locations and choosing the solution that has the lowest cost.\n",
        "\n",
        "Below, we show an example where our k-means implementation gets stuck in a local optimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB_zjZvGl2Qv"
      },
      "source": [
        "#@title Example of a non-optimal k-means solution\n",
        "X, y = make_blobs(n_samples=300, \n",
        "                  centers=4, \n",
        "                  cluster_std=0.60, \n",
        "                  random_state=0)\n",
        "\n",
        "centroids, labels = find_clusters(X, 4, rseed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjivk8PKKYKe"
      },
      "source": [
        "#Linearity\n",
        "\n",
        "You may have noticed that K-means divides the plane up linearly - the boundaries between clusters being straight lines.  This means that more complex, non-linear clusters may not be clustered properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p2K2Y9rFbay"
      },
      "source": [
        "#@title Generate Non-Linear Data\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X, y = make_circles(n_samples=400, \n",
        "                    factor=.3, \n",
        "                    noise=.05) #With no noise, this will \n",
        "                               #generate a perfect circle\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], s=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcklhevWLKdf"
      },
      "source": [
        "#@title Apply K-Means to Non-Linear Data\n",
        "centroids, y_hat, cost = k_means(X, n_clusters=2)\n",
        "\n",
        "# plot the result\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_hat, s=50) \n",
        "plt.scatter(centroids[:, 0], centroids[:, 1], \n",
        "            c='black', s=100, alpha=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrosUC2hLXEd"
      },
      "source": [
        "To deal with this situation, a more complex algorithm is needed.  One such algorithm is called **Spectral Clustering**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUxVP40dLt5D"
      },
      "source": [
        "#@title Spectral Clustering\n",
        "#@markdown Spectral Clustering creates a graph where data\n",
        "#@markdown that is similar to one another is connected to one \n",
        "#@markdown another.  This can be done in many different ways.\n",
        "#@markdown Here, we use a technique called 'Nearest Neighbors'.\n",
        "from sklearn.cluster import SpectralClustering\n",
        "\n",
        "model = SpectralClustering(n_clusters=2, \n",
        "                           affinity='nearest_neighbors')\n",
        "\n",
        "labels = model.fit_predict(X)\n",
        "\n",
        "# plot the results\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfIL0IDic1RZ"
      },
      "source": [
        "#K-Means for Image Compression\n",
        "\n",
        "An interesting use of the K-means algorithm is compression (sound, image, etc...).  We can simplify something like an image that has millions of colors into one with many fewer colors with only a mild reduction in quality.  This is sometimes kown as quantization and is used in other areas such as speech compression as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBIFDi-0dR17"
      },
      "source": [
        "#@title Load Image\n",
        "from sklearn.datasets import load_sample_image\n",
        "\n",
        "Image_Choice = 'Flower' #@param [\"Building\", \"Flower\"]\n",
        "if Image_Choice == 'Building':\n",
        "    img = load_sample_image(\"china.jpg\")\n",
        "else:\n",
        "    img = load_sample_image(\"flower.jpg\")\n",
        "\n",
        "ax = plt.axes(xticks=[], yticks=[])\n",
        "ax.imshow(img);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scU8BAs5eEiI"
      },
      "source": [
        "First, we need to reshape the data to be in the right format.  We're going to flatten it.  The data as given is in three dimensions - the x and y coordinates plus a third dimension for the color (containing the Red, Green, and Blue values)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8vqfhoOeAlg"
      },
      "source": [
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cD6V5R5ebTM"
      },
      "source": [
        "#@title Preprocess Image\n",
        "\n",
        "# Here we're going to normalize the data\n",
        "data = img / 255.0 # use 0...1 scale\n",
        "\n",
        "# Then reshape it\n",
        "data = data.reshape(img.shape[0] * img.shape[1], 3)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM_9wNgahhTB"
      },
      "source": [
        "Here we're going to use a variant of the K-means function we used above.  This MiniBatchKMeans is useful when the dataset is extremely large.  Optimization methods are used to speed up the run-time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGACOoR_e6MQ"
      },
      "source": [
        "#@title Cluster Image\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "#@markdown Compress the image into how many colors?\n",
        "Num_Colors = 1000 #@param {type:\"integer\"}\n",
        "\n",
        "#Cluster\n",
        "kmeans = MiniBatchKMeans(Num_Colors) #Useful when dataset is large\n",
        "kmeans.fit(data)\n",
        "new_colors = kmeans.cluster_centers_[kmeans.predict(data)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDy26JH7hx3Q"
      },
      "source": [
        "Once we have performed the K-means to segment the original 16 million colorspace into a smaller set (specified by num_colors above), we can now reconstruct our image using the smaller colorspace that we created using K-means."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FDoUArifJIU"
      },
      "source": [
        "#@title Reconstruct image with fewer colors\n",
        "img_recolored = new_colors.reshape(img.shape)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6),\n",
        "                       subplot_kw=dict(xticks=[], yticks=[]))\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].imshow(img)\n",
        "ax[0].set_title('16 million-color Original Image', size=16)\n",
        "ax[1].imshow(img_recolored)\n",
        "ax[1].set_title(str(Num_Colors) + '-color Image', size=16);"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}