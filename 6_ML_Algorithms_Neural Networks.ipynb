{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning - NN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGJTY9Lquo5IuscyVBk4rV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kpjaskie/SenSIP21/blob/main/6_ML_Algorithms_Neural%20Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nfLN5M1aGzK"
      },
      "source": [
        "# Neural Networks Introduction\n",
        "\n",
        "&#169; *Kristen Jaskie, June 2021*\n",
        "\n",
        "In this notebook, we will be using Neural Networks to solve both regression and classification problems using sklearn. \n",
        "\n",
        "Multi-layer Preceptrons are shallow neural networks (as opposed to deep learning networks).  This means that they usually only have a couple of hidden layers.  \n",
        "\n",
        "These networks generally have the following structure:\n",
        "\n",
        "\n",
        "![picture](https://www.tutorialspoint.com/tensorflow/images/multi_layer_perceptron.jpg)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b06ltZ2kaWcA"
      },
      "source": [
        "# First, we need to import the relavent libraries\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# To create simulated data\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# To create confusion matrices and metrics\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import seaborn as sns; sns.set()  # for plotting confusion matrices\n",
        "from tabulate import tabulate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVLzGfhln2Xe"
      },
      "source": [
        "# Neural Networks for Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irkGsOApoZb2"
      },
      "source": [
        "#@title Generate regression data\n",
        "\n",
        "Dataset_Size = 680 #@param {type:\"slider\", min:50, max:1000, step:10}\n",
        "Num_Total_Features = 66 #@param {type:\"slider\", min:2, max:100, step:1}\n",
        "Num_Useful_Features = 14 #@param {type:\"slider\", min:2, max:100, step:1}\n",
        "Noise = 0.16 #@param {type:\"slider\", min:0, max:2, step:0.01}\n",
        "\n",
        "#Let's make simple data to classify.\n",
        "X, y = make_regression(n_samples=Dataset_Size, \n",
        "                       n_features=Num_Total_Features,\n",
        "                       n_informative=Num_Useful_Features,\n",
        "                       noise=Noise,\n",
        "                       random_state=0)\n",
        "\n",
        "if(Num_Total_Features is 2):\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='PiYG');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbKqO9ItqY5m"
      },
      "source": [
        "#@title Split data into Train/Validation/Test sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Here, our training set is composed of 70% of the data and our validation\n",
        "#and test sets are each composed of 15% of the data\n",
        "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.3)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_remaining, y_remaining, test_size=0.5)\n",
        "\n",
        "#Combine training and validation sets for final classification when we're ready \n",
        "#for our final classification after validation training/testing.\n",
        "X_final_train = np.concatenate((X_train, X_val), axis=0)\n",
        "y_final_train = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "print(\"Training Size   = \", X_train.shape[0])\n",
        "print(\"Validation Size = \", X_val.shape[0])\n",
        "print(\"Test Size       = \",  X_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMqeSiMrsiRq"
      },
      "source": [
        "### Tune Hyperparameters\n",
        "\n",
        "We're definitely going to have to do some hyperparameter tuning on this - neural nets have more hyperparameters than other methods.\n",
        "\n",
        "Here, our hyperparameters consist of:\n",
        "* The number of hidden layers\n",
        "* The number of hidden nodes in each hidden layer\n",
        "* The activation function which can be 'identity’, ‘logistic’, ‘tanh’, or ‘relu’ ('relu' is fairly standard).  See the documentation for more details.\n",
        "* The type of solver which can be ‘lbfgs’, ‘sgd’, or ‘adam’.  'sgd' is standard gradiant descent, while 'adam' is a specialized version of gradient descent ('adam' is fairly standard)\n",
        "* The max_iter specifies the maximum number of iterations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkTofONEqt1N"
      },
      "source": [
        "#@title Tune Hyperparameters\n",
        "\n",
        "#@markdown Set the hidden layers and hidden nodes in the code\n",
        "Activation = 'relu' #@param [\"relu\",\"identity\", \"logistic\", \"tanh\"]\n",
        "Solver = 'adam' #@param [\"adam\",\"lbfgs\", \"sgd\"]\n",
        "Maximum_Iterations = 4990 #@param {type:\"slider\", min:0, max:10000, step:100}\n",
        "\n",
        "# The number of nodes in each hidden layer is specified in this first field\n",
        "NN_model_train = MLPRegressor(hidden_layer_sizes=(25, ), \n",
        "                              activation=Activation, \n",
        "                              solver=Solver, \n",
        "                              max_iter=Maximum_Iterations)\n",
        "\n",
        "NN_model_train.fit(X_train, y_train)\n",
        "nn_y_hat_train = NN_model_train.predict(X_train)\n",
        "nn_y_hat_val = NN_model_train.predict(X_val)\n",
        "\n",
        "nn_MSE_train = mean_squared_error(y_train, nn_y_hat_train) / y_train.shape[0]\n",
        "nn_MSE_val = mean_squared_error(y_val, nn_y_hat_val) / y_val.shape[0]\n",
        "print('Training Error: ', nn_MSE_train)\n",
        "print('Validation Error: ', nn_MSE_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ul6nZzBtIRT"
      },
      "source": [
        "#@title Automating Hyperparameter Selection using Grid Search\n",
        "\n",
        "Activation = 'relu' #@param [\"relu\",\"identity\", \"logistic\", \"tanh\"]\n",
        "Solver = 'adam' #@param [\"adam\",\"lbfgs\", \"sgd\"]\n",
        "Maximum_Iterations = 10000 #@param {type:\"slider\", min:0, max:10000, step:100}\n",
        "\n",
        "#Try different numbers of hidden nodes in each layer \n",
        "hidden_nodes = [2, 4, 6, 10, 15, 25, 50]\n",
        "\n",
        "#Let's try for just one hidden layer\n",
        "for hl in hidden_nodes:\n",
        "  NN_model_train = MLPRegressor(hidden_layer_sizes=(hl, ), \n",
        "                              activation=Activation, \n",
        "                              solver=Solver, \n",
        "                              max_iter=Maximum_Iterations)\n",
        "  \n",
        "  NN_model_train.fit(X_train, y_train)\n",
        "  nn_y_hat_train = NN_model_train.predict(X_train)\n",
        "  nn_y_hat_val = NN_model_train.predict(X_val)\n",
        "\n",
        "  nn_MSE_train = mean_squared_error(y_train, nn_y_hat_train) / y_train.shape[0]\n",
        "  nn_MSE_val = mean_squared_error(y_val, nn_y_hat_val) / y_val.shape[0]\n",
        "  print('One hidden layer with ', hl, ' nodes. Training MSE: ', round(nn_MSE_train,3), \n",
        "        'Validation MSE: ', round(nn_MSE_val,3))\n",
        "\n",
        "#Now we'll try two hidden layers\n",
        "for hl in hidden_nodes:\n",
        "  NN_model_train = MLPRegressor(hidden_layer_sizes=(hl, hl), \n",
        "                              activation=Activation, \n",
        "                              solver=Solver, \n",
        "                              max_iter=Maximum_Iterations)\n",
        "  \n",
        "  NN_model_train.fit(X_train, y_train)\n",
        "  nn_y_hat_train = NN_model_train.predict(X_train)\n",
        "  nn_y_hat_val = NN_model_train.predict(X_val)\n",
        "\n",
        "  nn_MSE_train = mean_squared_error(y_train, nn_y_hat_train) / y_train.shape[0]\n",
        "  nn_MSE_val = mean_squared_error(y_val, nn_y_hat_val) / y_val.shape[0]\n",
        "  print('Two hidden layers with ', hl, ' nodes each. Training MSE: ', round(nn_MSE_train,3), \n",
        "        'Validation MSE: ', round(nn_MSE_val,3))\n",
        "  \n",
        "#Now we'll try three hidden layers\n",
        "for hl in hidden_nodes:\n",
        "  NN_model_train = MLPRegressor(hidden_layer_sizes=(hl, hl, hl), \n",
        "                              activation=Activation, \n",
        "                              solver=Solver, \n",
        "                              max_iter=Maximum_Iterations)\n",
        "  \n",
        "  NN_model_train.fit(X_train, y_train)\n",
        "  nn_y_hat_train = NN_model_train.predict(X_train)\n",
        "  nn_y_hat_val = NN_model_train.predict(X_val)\n",
        "\n",
        "  nn_MSE_train = mean_squared_error(y_train, nn_y_hat_train) / y_train.shape[0]\n",
        "  nn_MSE_val = mean_squared_error(y_val, nn_y_hat_val) / y_val.shape[0]\n",
        "  print('Three hidden layers with ', hl, ' nodes each. Training MSE: ', round(nn_MSE_train,3), \n",
        "        'Validation MSE: ', round(nn_MSE_val,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kbFqeRUyIXQ"
      },
      "source": [
        "#@title Test Final Model Performance\n",
        "\n",
        "Activation = 'relu' #@param [\"relu\",\"identity\", \"logistic\", \"tanh\"]\n",
        "Solver = 'adam' #@param [\"adam\",\"lbfgs\", \"sgd\"]\n",
        "Maximum_Iterations = 7300 #@param {type:\"slider\", min:0, max:10000, step:100}\n",
        "\n",
        "NN_model_train = MLPRegressor(hidden_layer_sizes=(hl, ), \n",
        "                              activation=Activation, \n",
        "                              solver=Solver, \n",
        "                              max_iter=Maximum_Iterations)\n",
        "  \n",
        "NN_model_train.fit(X_final_train, y_final_train)\n",
        "nn_y_hat_train = NN_model_train.predict(X_final_train)\n",
        "nn_y_hat_test = NN_model_train.predict(X_test)\n",
        "\n",
        "nn_MSE_train = mean_squared_error(y_final_train, nn_y_hat_train) / y_final_train.shape[0]\n",
        "nn_MSE_test = mean_squared_error(y_test, nn_y_hat_test) / y_test.shape[0]\n",
        "print('Training Error: ', nn_MSE_train)\n",
        "print('Final Testing Error: ', nn_MSE_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wKzJsVRzTJI"
      },
      "source": [
        "# Neural Networks for Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylo49S-Gi0SM"
      },
      "source": [
        "#@title Generate binary data\n",
        "\n",
        "Dataset_Size = 840 #@param {type:\"slider\", min:50, max:1000, step:10}\n",
        "Standard_Deviation = 0.75 #@param {type:\"slider\", min:0, max:2, step:0.01}\n",
        "\n",
        "#Let's make simple data to classify.\n",
        "X, y = make_blobs(n_samples=Dataset_Size, \n",
        "                  centers=2, \n",
        "                  random_state=0, \n",
        "                  cluster_std=Standard_Deviation)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='PiYG');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlI_5XFIIN2H"
      },
      "source": [
        "From this, we can clearly see the two well defined classes of data - red and green.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtX6tE-wlty2"
      },
      "source": [
        "#Here, our training set is composed of 70% of the data and our validation\n",
        "#and test sets are each composed of 15% of the data\n",
        "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.3)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_remaining, y_remaining, test_size=0.5)\n",
        "\n",
        "#Combine training and validation sets for final classification when we're ready \n",
        "#for our final classification after validation training/testing.\n",
        "X_final_train = np.concatenate((X_train, X_val), axis=0)\n",
        "y_final_train = np.concatenate((y_train, y_val), axis=0)\n",
        "\n",
        "print(\"Training Size   = \", X_train.shape[0])\n",
        "print(\"Validation Size = \", X_val.shape[0])\n",
        "print(\"Test Size       = \",  X_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvSo7oXimgqb",
        "cellView": "form"
      },
      "source": [
        "#@title Visualize Sets\n",
        "\n",
        "from matplotlib import gridspec\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, \n",
        "                       gridspec_kw={\n",
        "                           'width_ratios': [2, 2, 2],\n",
        "                           'height_ratios': [1]},\n",
        "                       figsize=(15,5))\n",
        "\n",
        "ax[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=50, cmap='PiYG');\n",
        "ax[0].set_title('Training Set')\n",
        "\n",
        "ax[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, s=50, cmap='PiYG');\n",
        "ax[1].set_title('Validation Set')\n",
        "\n",
        "ax[2].scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap='PiYG');\n",
        "ax[2].set_title('Testing Set')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe4aNWPLNJug"
      },
      "source": [
        "### NNs on Simulated Data\n",
        "\n",
        "We're definitely going to have to do some hyperparameter tuning on this - neural nets have more hyperparameters than most other methods.\n",
        "\n",
        "Here, our hyperparameters consist of:\n",
        "* The number of hidden layers\n",
        "* The number of hidden nodes in each hidden layer\n",
        "* The activation function which can be ‘identity’, ‘logistic’, ‘tanh’, or ‘relu’ ('relu' is fairly standard).  See the documentation for more details.\n",
        "* The type of solver which can be ‘lbfgs’, ‘sgd’, or ‘adam’.  'sgd' is standard gradiant descent, while 'adam' is a specialized version of gradient descent ('adam' is fairly standard)\n",
        "* The max_iter specifies the maximum number of iterations.\n",
        "\n",
        "The following code automatically tests a variety of hidden layers and hidden nodes though it uses the standard 'relu' activation function and 'adam' solver.  Feel free to play with these as well.\n",
        "\n",
        "If you want to see more options, take a look at the documentation:\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html?highlight=mlp#sklearn.neural_network.MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpMOcnCNz5ce"
      },
      "source": [
        "#@title Tune Hyperparameters\n",
        "\n",
        "#@markdown Set the hidden layers and hidden nodes in the code\n",
        "Activation = 'relu' #@param [\"relu\",\"identity\", \"logistic\", \"tanh\"]\n",
        "Solver = 'adam' #@param [\"adam\",\"lbfgs\", \"sgd\"]\n",
        "Maximum_Iterations = 4990 #@param {type:\"slider\", min:0, max:10000, step:100}\n",
        "\n",
        "# The number of nodes in each hidden layer is specified in this first field\n",
        "NN_classify_model = MLPClassifier(hidden_layer_sizes=(10,), \n",
        "                                     activation=Activation, \n",
        "                                     solver=Solver, \n",
        "                                     max_iter=Maximum_Iterations)\n",
        "\n",
        "\n",
        "NN_classify_model.fit(X_train, y_train)\n",
        "class_y_hat_train = NN_classify_model.predict(X_train)\n",
        "class_y_hat_val = NN_classify_model.predict(X_val)\n",
        "\n",
        "train_accuracy = accuracy_score(y_train, class_y_hat_train)\n",
        "val_accuracy = accuracy_score(y_val, class_y_hat_val)\n",
        "print('Training Accuracy: ', train_accuracy)\n",
        "print('Validation Accuracy: ', val_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwYf9XrabQ1R"
      },
      "source": [
        "#@title Automating Hyperparameter Selection using Grid Search\n",
        "\n",
        "#Try different hyperparameter values\n",
        "hidden_nodes = [2, 4, 6, 10, 15, 25, 50]\n",
        "hidden_layers = [1, 2, 3]\n",
        "nn_activation = 'relu'\n",
        "nn_solver = 'adam'\n",
        "nn_max_iter = 1000\n",
        "\n",
        "for layer in hidden_layers:\n",
        "    for node in hidden_nodes:\n",
        "        if layer is 1:\n",
        "            NN_model = MLPClassifier(hidden_layer_sizes=(node,), \n",
        "                                     activation=nn_activation, \n",
        "                                     solver=nn_solver, \n",
        "                                     max_iter=nn_max_iter)\n",
        "        elif layer is 2:\n",
        "            NN_model = MLPClassifier(hidden_layer_sizes=(node, node), \n",
        "                                     activation=nn_activation, \n",
        "                                     solver=nn_solver, \n",
        "                                     max_iter=nn_max_iter)\n",
        "        elif layer is 3:\n",
        "            NN_model = MLPClassifier(hidden_layer_sizes=(node, node, node), \n",
        "                                     activation=nn_activation, \n",
        "                                     solver=nn_solver, \n",
        "                                     max_iter=nn_max_iter)\n",
        "        NN_model.fit(X_train, y_train)\n",
        "        #y_hat_nn = NN_model.predict(X_val)\n",
        "\n",
        "        class_y_hat_train = NN_classify_model.predict(X_train)\n",
        "        class_y_hat_val = NN_classify_model.predict(X_val)\n",
        "\n",
        "        train_accuracy = accuracy_score(y_train, class_y_hat_train)\n",
        "        val_accuracy = accuracy_score(y_val, class_y_hat_val)\n",
        "        print(layer, ' hidden layers, ', node, ' nodes each. Training accuracy: ', \n",
        "              round(train_accuracy,3), 'Validation accuracy: ', round(val_accuracy,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqgyLwFZ2RVU"
      },
      "source": [
        "#@title Test Final Model Performance\n",
        "\n",
        "Activation = 'relu' #@param [\"relu\",\"identity\", \"logistic\", \"tanh\"]\n",
        "Solver = 'adam' #@param [\"adam\",\"lbfgs\", \"sgd\"]\n",
        "Maximum_Iterations = 7300 #@param {type:\"slider\", min:0, max:10000, step:100}\n",
        "\n",
        "NN_classify_model = MLPClassifier(hidden_layer_sizes=(10,), \n",
        "                                     activation=Activation, \n",
        "                                     solver=Solver, \n",
        "                                     max_iter=Maximum_Iterations)\n",
        "\n",
        "\n",
        "NN_classify_model.fit(X_final_train, y_final_train)\n",
        "class_y_hat_train = NN_classify_model.predict(X_final_train)\n",
        "class_y_hat_test = NN_classify_model.predict(X_test)\n",
        "\n",
        "train_accuracy = accuracy_score(y_final_train, class_y_hat_train)\n",
        "test_accuracy = accuracy_score(y_test, class_y_hat_test)\n",
        "print('Training Accuracy: ', train_accuracy)\n",
        "print('Test Accuracy: ', test_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rM9YKUARh2C"
      },
      "source": [
        "#@title Evaluate Final Model\n",
        "\n",
        "plt.figure(0)\n",
        "plt.scatter(X_final_train[:, 0], X_final_train[:, 1], c=y_final_train, s=50, cmap='PiYG', alpha=0.2);\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=class_y_hat_test, s=50, cmap='PiYG');\n",
        "plt.show()\n",
        "\n",
        "accuracy_nn_final = round(accuracy_score(y_test, class_y_hat_test), 2)\n",
        "precision_nn_final = round(precision_score(y_test, class_y_hat_test), 2)\n",
        "recall_nn_final = round(recall_score(y_test, class_y_hat_test), 2)\n",
        "fscore_nn_final = round(f1_score(y_test, class_y_hat_test), 2)\n",
        "\n",
        "print(tabulate([['Accuracy', accuracy_nn_final], \n",
        "                ['Precision', precision_nn_final],\n",
        "                ['Recall', recall_nn_final],\n",
        "                ['F-score', fscore_nn_final],\n",
        "                [' ', ' ']], \n",
        "               headers=['Metric', 'Value']))\n",
        "\n",
        "CM_nn = confusion_matrix(y_test, class_y_hat_test)\n",
        "\n",
        "ax = sns.heatmap(CM_nn.T, square=True, annot=True, \n",
        "                 fmt='d', cbar=False, cmap=\"Blues\")\n",
        "\n",
        "ax.set_xticklabels(('Red Class','Green Class'))\n",
        "ax.set_yticklabels(('Red Class','Green Class'), \n",
        "    rotation=0, fontsize=\"10\", va=\"center\")\n",
        "\n",
        "plt.xlabel(\"True Labels\")\n",
        "plt.ylabel(\"Predicted Labels\");\n",
        "plt.title(\"NN Final Test Confusion Matrix\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}